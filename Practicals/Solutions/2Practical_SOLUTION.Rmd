---
title: "Practical: Simple linear regression"
subtitle: "Physalia workshop on GLMs"
author: "Bert van der Veen"
output: html_document
editor_options: 
  chunk_output_type: console
---

# R functions

The second presentation of the workshop focused on simple linear regression. In this practical, we will simulate normally distributed data again, but this time following a linear regression. For this you will need the following functions:

- `rnorm`: (pseudo) random number generator (RNG) based on the binomial distribution
- `set.seed`: controls the RNG to ensure reproducibility
- `quantile`: calculates the quantiles of a vector
- `plot`: draws all kinds of plots (controlled by `type`), but usually a scatterplot
- `abline`: draws a straight line through a plot. Accepts a model as input (only for simple linear regression)
- `lm`: fits a linear regression to data. Takes as first argument a formula of the form y~x, and second a dataset.
- `summary`: provides a summary table for a (linear) model
- `confint`: calculates (estimated) confidence intervals for the parameters of a model

If you do not remember what these functions do, or what arguments they take, you can look it up by typing `?functionName` in the console. That will show you the help page of the function, which hopefully clarifies things. If it does not, try a search engine to find online resources, or ask someone. Almost always your question has been asked (and answered) by someone else before. Some hints are given with questions **in bold**.

# R-packages

There are various functions to help you organize model outputs, or plot the regression line. For example, `display` in the \texttt{arm} R-package can help to retrieve a cleaner summary output from the model. The `visreg` function in the \texttt{visreg} R-package provides an easy way to quickly plot your regression with confidence intervals.

# Description

In the previous practical we focused on estimating the mean of a normally distributed population, based on differently sized samples. Hopefully, you discovered that as the sample size increases we become more certain of the "best" estimate for the true parameter (i.e., that it is less likely to change if we collect data again).

We estimated the population mean using the sample average, which you learned in the second presentation is the maximum likelihood estimator for the population mean, in case of a normally distributed population at least. Usually, there are more components that determine how observations structurally differ from each other. Observations might change along a covariate, or because they are part of two different groups.

In this practical we continue where we left off at the last practical. We will simulate normally distributed data, but this time assuming that was generated by a simple linear regression. Then, we will estimate the parameters of the simple linear regression, and try to understand how linear regression in applied in \texttt{R}, what outputs it gives, and how those change with the sample size and standard deviation of the residuals.

**Continue simulating normally distributed data, but now where the mean is different for each observation**. Don't forget to use `set.seed` for reproducibility. You can answer the questions below for guidance.

# Tasks

1. If you were to write down a simple linear regression for the last simulation of the first practical (in \texttt{R} code), what would it look like? **Hint: the estimator for the intercept of a simple linear regression without covariates is the sample average**

```{r}
#alpha+epsilon
```

2. Fit the linear regression to your simulation, and extract the estimate for the standard deviation **Hint: you can do this with the `sigma` function**. What is the estimate, and what did you simulate?
```{r}
y <- rnorm(1000, mean = 6, sd = 5)
model1 <- lm(y~1)
coef(model1);confint(model1)
sigma(model1)
```

---

Now we will simulate a simple linear regression, with a continuous covariate. We can do this in the following steps:

- Simulate a normally distributed covariate
- Choose a value for the true intercept $\alpha$ and the true slope parameter $\beta$
- Calculate $\mu_i = \alpha +x_i\beta$
- Simulate with $\mu_i$ as the (vector of) means

---

3. Simulate data from a simple linear regression, and fit a simple linear regression to the data. What are the parameter estimates? **Hint: summary(model) gives you a table of coefficients and estimates of their uncertainty.**

```{r}
x <- rnorm(20)
alpha = 1
beta = 2
y <- rnorm(20, alpha+beta*x)
model2 <- lm(y~x)
```

4. Plot the linear regression. **Hint: use the `plot` and `abline` functions.**

```{r}
plot(y~x);plot(x,y)
abline(model2)
```

5. Simulate data again, but now with a categorical covariate, and plot the linear regression again. Is this a good visual presentation of this model? **Hint: sample(x = 0:1, size = 10, replace = TRUE) simulates a categorical covariate with two groups**

```{r}
x <- rbinom(20,size = 1,prob = 0.5)
model3 <- lm(y~x)
plot(y~x)
abline(model3)
```

6. Use `t.test` to perform a t-test with the simulated $y_i$ and categorical $x_i$. Compare the result to the linear regression output.

```{r}
t.test(y[x==0], y[x==1])
```

7. Can you draw any conclusions about the estimated uncertainty of the parameter estimates? **Hint: you can retrieve confidence intervals for the parameters with the `confint` function.**

```{r}
confint(model3)
```

8. Simulate data with different sample sizes and standard deviations. How does that affect the uncertainty of parameter estimates?

```{r}
# gets smaller with larger n
# gets larger with larger standard deviation
```

9. Calculate confidence intervals via simulation similarly to the first practical instead: simulate data, fit a linear regression, store the parameter estimates, repeat. How do these confidence intervals compare to those from the `confint` function? **Hint: use the `quantile` function.**

```{r}
dolm <- function(x){
  ynew <- as.matrix(simulate(model3))
  coef(lm(ynew~x))
}
apply(t(replicate(1000,dolm(x))),2,quantile,probs=c(0.025,0.975))

# less accurate especially for small nr of simulations
```

10. How do you interpret a confidence interval? **Hint: it is not the range of the true parameter, nor the range of the true parameter with .95 probability**

```{r}
# range that contains the true parameter 95% of the time
```

