---
title: "Practical: model comparison"
subtitle: "Physalia workshop on GLMs"
author: "Bert van der Veen"
output: html_document
---

# R functions

In the last practical we used information criteria to compare binomial glms. In this practical we return to those models and try to figure out which model fits "best" to the data. For this you will might the following functions:

- `AIC`: extracts Akaike's Information Criteria from a model
- `BIC`: extracts Schwarz's Bayesian Information Criteria from a model
- `anova`: performs a Likelihood Ratio Test
- `deviance`: the unit deviance for a GLM
- `summary`: provides a summary table for a (linear) model
- `confint`: calculates (estimated) confidence intervals for the parameters of a model
- `predict`: calculates the predicted values from a regression, potentially for new $X$
- `pairs`: construct a panel of plots for a dataset
- `relevel`: changes the reference category for a factor

and perhaps some of the functions or R packages used in the previous practicals.

If you do not remember what these functions do, or what arguments they take, you can look it up by typing `?functionName` in the console. That will show you the help page of the function, which hopefully clarifies things. If it does not, try a search engine to find online resources, or ask someone. Almost always your question has been asked (and answered) by someone else before. Some hints are given with questions **in bold**.

# R-packages

Some of the R-packages and function that might be helpful in this practical include:

- `AICcmodavg::AICc`: extract corrected AIC from a model. Alternatively `MuMIN::AICc`
- `AICcmodavg::aictab`: creates a model selection table 
- `AICcmodavg::bictab`: creates a model selection table
- `MuMIn::dredge`: automatic model selection
- `car::Anova` Type-II or Type-III LRTs
- `DHARMa::SimulateLRT`: Likelihood ratio test via simulation
- `DescTools::PseudoR2` calculates various $R^2$ measures for GLMs
- `glmtoolbox::adjR2` calculates deviance $R^2$ for GLMs
- `MuMIn::r.squaredLR`: LRT $R^2$

# Description

In exploratory analysis, information criteria help us in finding the "best" model in a candidate set of models. However, if we want to test a hypothesis, information criteria are not a suitable tool. Instead, in LMs we perform an analysis of (co)variance, and in GLMs an analysis of deviance. Both are examples of Likelihood Ratio Tests (LRT). Note, that LRT requires models to be *nested*, unlike information criteria.

The goal of this practical is to go over the models that you fitted during the last practicals (or for the multiple linear regression practical) and determine for your dataset of choice, and do a confirmatory analysis. You can also use $R^2$ measures for support. Again you can choose one of the following datasets from the binomial regression practical:

1. Lizards: Lizard counts, used in the presentation this morning, includes a count of two lizard species and multiple categorical covariates (Schoener 1970)
2. Moose: Data on the detection of GPS collared moose by helicopter. This includes thee covariates: year of the survey, amount of visual obstruction in a 10m radius of the moose, number of moose in the sighted group (Giudice et al. 2012)
3. deposit: Number of insects killed and exposed as a function of insecticide types with different doses (deposit)
4. wells: Water quality of contaminated wells (y vs n) in New York with variables of surrounding land use, sewer usage, and nitrate and chloride concentration (Eckhard et al. 1989)
5. Alternatively, you can simulate binomial distributed responses

Or any of the dataset for the multiple linear regression example. Don't forget to use `set.seed` for reproducibility if you do simulations. You can answer the questions below for guidance. You can construct the plots from the presentation yourself with the aforementioned functions, or use the `plot` function on your model.

# Tasks

1. What is the difference between confirmatory and exploratory analysis?

```{r}
lizards <- read.csv("/home/bertv/GLM-workshop/data/lizards.csv")
lizards <- lizards[-11,]
# Confirmatory: we test an hypothesis
# Exploratory: we compare models in a set
```

2. Can you calculate deviance by hand for the binomial distribution? *Hint: it is included in the slides from yesterday evening *

```{r}
y <- lizards$grahami
N <- rowSums(lizards[,c("grahami","opalinus")])
model <- glm(cbind(grahami, opalinus)~Time+Site, family="binomial", data =lizards)
pred <- predict(model, type = "response")

tempres <- (N-y)/(N-pred)
tempres <- ifelse(is.infinite(log(tempres)), 0, log(tempres))
2*sum(y*log(y/pred) + (N-y)*tempres)
```


3. What is the benefit of LRT over information criteria for confirmatory analysis?

```{r}
# Information cannot give us a yes or no answer to which model is better
# Information criteria are not a formal test
# LRT is a formal test; gives us a p-value
```

4. Formulate a hypothesis, fit two models, and use `anova` to compare them. Do you accept or reject the alternative hypothesis?

```{r}
model1 <- glm(cbind(grahami, opalinus)~Time*Site+Diameter+Height, family="binomial", data =lizards)
anova(model, model1, test = "LRT") # p<0.05, so we accept the alternative hypothesis
```

5. Use `anova` with the more complex of the two models in 3. Compare the results of `anova(model, test = "Chisq")` to `car::Anova(model, type = ...)` how do the results of the two functions differ?

```{r}
anova(model1, test = "LRT")

model <- glm(cbind(grahami, opalinus)~Time+Site+Diameter+Height, family="binomial", data =lizards)
car::Anova(model1, type = "II") 
anova(model,model1, test = "LRT") # same as type II anova last step
```

6. Why are the results different from the two anova functions?

```{r}
# The results are different: here the models are compared sequentially by adding a single variable.
# The previous hypothesis test performed a comparison for adding two variables.
```

7. Does AIC/AICc/BIC give you the same or similar results?

```{r}
# fit models from anova table
model2 <- glm(cbind(grahami,opalinus)~1, family = "binomial", data = lizards)
model3 <- glm(cbind(grahami,opalinus)~Time, family = "binomial", data = lizards)
model4 <- glm(cbind(grahami,opalinus)~Time+Site, family = "binomial", data = lizards)
model5 <- glm(cbind(grahami,opalinus)~Time+Site+Diameter, family = "binomial", data = lizards)
model6 <- glm(cbind(grahami,opalinus)~Time+Site+Diameter+Height, family = "binomial", data = lizards)

# Let's have a look with aic first
modelist <- list(one=model2, two=model3, three=model4, four=model5, five=model6)
AICcmodavg::aictab(modelist)
AICcmodavg::bictab(modelist)
```

8. Perform model comparison for different datasets.