---
title: "Sampling and variation"
subtitle: "Physalia workshop on GLMs"
author: "Bert van der Veen"
output: html_document
---

# Background

A very important component in statistics, is the ability to quantify how repeatedly collecting data can affect the results that we find.

Since we cannot go out and collect data multiple times during the workshop, we will use simulations to explore sampling variation.

Simulations are a way of "collecting data" artificially. If we make an assumption what distribution the data comes from, we can explore how

things as sample size, and different values for the parameters affect our ability to find a signal in the data.

R has a range of functions to simulate from a variety of distributions, including those corresponding to Generalised Linear Models.

Here, we will first focus on the **normal** distribution, which is defined by two parameters: the mean $\mu$ determines its location and the standard deviation $\sigma$ determines its scale. A smaller value for $\sigma$ makes the distribution look more narrow, and a larger value more wide.

For each distribution, R uses the same name convention:

- `dnorm`: the density function, this is used to calculate the likelihood
- `pnorm`: the distribution function, this is used to calculate probabilities
- `qnorm`: the quantile function, this is used to find the quantile for an observation
- `rnorm`: the random number generator for the distribution

You can find more information about each of these by writing `?dnorm`.

In this practical we will use the `rnorm` function, which has arguments `n`, `mean`, and `sd`. `n` is the number of observations we want to generate, and the other two arguments are the parameters for the distribution. 

So, for example, we can generate a single observation from a normal distribution with zero mean and standard deviation one as follows:

```{r}
(y <- rnorm(1, mean = 0, sd = 1))
```

If we do this again, we do not get the same value:

```{r}
(y <- rnorm(1, mean = 0, sd = 1))
```

We can plot the generated observation with a histogram:
```{r, fig.width = 10}
hist(y)
```

this looks a bit weird because we only generated one observation. The larger `n`, the more clearly we can distinguish the normal distribution:

```{r, fig.show = "hold", fig.width = 10}
par(mfrow=c(1, 3))
(y <- rnorm(5, mean = 0, sd = 1))
hist(y, main = "n = 5")
(y <- rnorm(10, mean = 0, sd = 1))
hist(y, main = "n = 10")
(y <- rnorm(15, mean = 0, sd = 1))
hist(y, main = "n = 15")
```
So, if we set `n` to a very large number, we can see the normal distribution appear:

```{r, fig.show = "hold", fig.width = 10}
par(mfrow=c(1, 1))
y <- rnorm(1000, mean = 0, sd = 1)
hist(y, breaks = 50, main = "n = 1000")
```

To get a completely smooth distribution from a histogram, `n` would have to be extremely large. Luckily R can help us out:

```{r, fig.width = 10}
curve(dnorm, from = -5, to = 5)
```

Different settings for `n`, `mean`, and `sd` represent real-life situations under which we may collect data. `n` is the sample size: small values represent situations where we have collected little data. `mean` we will think of as "the effect" for now; small values are harder to find than large values. `sd` represents the amount of noise in the data that obscures the effect; larger `sd` makes the effect harder to find.

# Part 1

The goal is for you to get a little more comfortable with the concept of sampling variation, the normal distribution, and to interact with each other.

Here is what I want you to do:

1. Get familiar with the `rnorm` and `hist` functions. By trying them out, or by looking at their help pages via `?...`
2. Simulate some data with `rnorm`, try out different settings for the arguments `n`, `mean`, and `sd`
3. Decide on a "final" setting for the three arguments, and plot a histogram of the simulation
4. Show the histogram in zoom, we will discuss (do not tell anyone the values of `n`, `mean`, and `sd` that you used)

# Part 2

<details><summary> Open after discussion on zoom</summary>

After the discussion on zoom you received the data that a different group simulated. You can load the data by:

```{r, eval = FALSE}
load(file="y.RData")
```

just make sure to put the data in the place where R looks for it. You can check where this is by using:

```{r, eval = FALSE}
getwd()
```

For the normal distribution, the maximum likelihood estimators for $\mu$ and $\sigma$ are the mean of the data and the standard deviation in the data. We will look a bit more into this during the next presentation. For now, you can calulate these with the `mean` and `sd` functions:

```{r}
mean(y)
sd(y)
```

both of them only take one argument; the data, so that is easy! We can check the number of observations using the `length` function:

```{r}
length(y)
```

After you calculated the mean, standard deviation, and determined the number of observations in the data, you should discuss with each other. Remember, these are the estimates $\hat{\mu}$ and $\hat{\sigma}$ of the true parameters $\mu$ and $\sigma$, and estimators have uncertainty! Given the number observations, and the amount of noise (indicated by $\sigma$), try answering these questions:

1. Is the number of observations large enough to accurately estimate $\mu$ and $\sigma$?
2. Which of the groups do you think simulated the data? Think back to the histograms you saw on zoom.

After a bit we will present these values on zoom, and see if each group can identify who got their simulations!

</details>

[If you finish early, there is also the practical from last year.](https://htmlpreview.github.io/?https://github.com/BertvanderVeen/GLM-workshop/blob/main/Practicals/2024/1Practical.html)
