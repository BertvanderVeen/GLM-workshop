---
title: "Practical: Multiple linear regression"
subtitle: "Physalia workshop on GLMs"
author: "Bert van der Veen"
output: html_document
---

# Background

Yesterday, we learned about sampling variation, uncertainty, t-tests and simple linear regression. We did this by simulating data from a normal distribution. In this practical we will focus on linear regression, for which we can also simulate some data:

```{r}
x <- runif(20, 3, 5)
y <- rnorm(20, mean = 1 + x*2)
```

In this simulation, $\textbf{x}$ is a predictor variable simulated between 3 and 5 (so it takes only positive values beween those numbers), and $\textbf{y} = \alpha + \textbf{x}\beta + \boldsymbol{\epsilon}$ is a response variable with intercept $\alpha = 1$ (where $x = 0$), slope for the predictor $\beta = 2$, and error term $\boldsymbol{\epsilon}$ with mean zero and standard deviation one.

Every time we collect data, there is some randomness involved. This affects the exact value for the parameter estimate we are able to find, and might somewhat affect our conclusion. For the t-test, this means we might not find a difference in the means of groups, and in the context of simple linear regression it might mean that we do not always find an effect statistically significant. Moreover, it means that sometimes we might conclude a negative relationship between the response and explanatory variable, and other times a positive relationship.

Here, if we fit a linear regression to the simulated data, this might show as an inaccurate estimate for one of the parameters; perhaps it is negative while we simulated it to be positive. Similarly, if we simulate another variable $\textbf{z}$ that $\textbf{y}$ is not truly determined by, it might still show as statistically significant by chance! We might also be able to see this visually:

```{r}
z <- rnorm(20)

par(mfrow=c(1,2))
plot(y~x)
plot(y~z)
```

Can you imagine drawing a line through the points in the plots?

Either way, it shows us how a multiple linear regression is fitted in R:

```{r}
summary(reg<-lm(y~x+z))
```

A confidence interval helps us to summarize this information. Unfortunately, the confidence interval is also affected by sampling variation (it is calculated from the data after all). Still, it is one of the few tools that we possess to quantify how much our answer might change, so we apply it to the best our ability, despite its flaws. The `confint` function lets us extract the confidence intervals from a linear model:

```{r}
confint(reg)
```

Although these are labelled following the predictors in the model, they correspond to the parameter estimates for those predictor variables. If a confidence interval crosses zero, that means we are not quite sure if the effect of a predictor on the response variable is positive or negative. Ultimately, we can say very little from this single confidence interval per parameter estimate based on the data. Generally speaking, if a confidence interval is narrow it means that we have more evidence in the data for this effect, and if it is wide it means there is little evidence (evidence in terms of available information).

For this practical you may need the following functions:

- `plot`: draws all kinds of plots (controlled by `type`), but usually a scatterplots
- `abline`: draws a straight line through a plot. Accepts a model as input (only for simple linear regression)
- `lm`: fits a linear regression to data
- `summary`: provides a summary table for a (linear) model
- `confint`: calculates (estimated) confidence intervals for the parameters of a model
- `predict`: calculates the predicted values from a regression, potentially for new $x$
- `pairs`: construct a panel of plots for a dataset
- `relevel`: changes the reference category for a factor

If you do not remember what these functions do, or what arguments they take, you can look it up by typing `?...` in the console. That will show you the help page of the function, which hopefully clarifies things. If it does not, try a search engine to find online resources, or ask someone. Almost always your question has been asked (and answered) by someone else before. Some hints are given with questions **in bold**.

There are various functions to help you organize model outputs, or plot the regression line. For example, `display` in the `arm` R-package can help to retrieve a cleaner summary output from the model. The `visreg` function in the \texttt{visreg}, or the `allEffects` function in the `effects` R-package or the `ggeffect` function in the `ggeffects` package provide easy ways to quickly plot your regression. The `coefplot` function in the `arm` function makes plots of coefficients.

# Datasets

You can find the datasets in the "data" folder of the github repository.

1. [nickel.expand](https://raw.githubusercontent.com/BertvanderVeen/GLM-workshop/refs/heads/main/data/nickel.expand.csv): Lung cancer due to nickel exposure in South Wales [Breslow and Day (1987)](https://pubmed.ncbi.nlm.nih.gov/3329634/)
2. [dmn](https://raw.githubusercontent.com/BertvanderVeen/GLM-workshop/refs/heads/main/data/dmn.csv): de novo mutations in children [Halldorsson et al. (2019)](https://www.science.org/doi/10.1126/science.aau1043), see also [this page](https://mccoy-lab.github.io/hgv_modules/setup.html)
2. [ISIT](https://raw.githubusercontent.com/BertvanderVeen/GLM-workshop/refs/heads/main/data/ISIT.csv): Pelagic bioluminescence ("Sources") along a depth gradient in different seasons ([Gillibrand et al. 2007](https://www.int-res.com/abstracts/meps/v341/p37-44/))
4. [TeethNitrogen](https://raw.githubusercontent.com/BertvanderVeen/GLM-workshop/refs/heads/main/data/TeethNitrogen.csv): Nitrogen isotope ratios in teeth of stranded whales ([Mendes et al. 2007](https://link.springer.com/article/10.1007/s00442-006-0612-z))

# Part 1

So far we have not used the `data` argument of the `lm` object. To fit a linear regression with a dataset, you will have to do that.

Here is what I want you to do:

1. Pick a dataset and identify the response variable $\textbf{y}$.
2. Identify potential predictor variables $\textbf{x}$ for a multiple linear regression $\textbf{x}$.
3. Explore the data, a good analysis always start with exploratory analysis. Use `plot` to visualize the relationship between $\textbf{y}$ and one or two predictors.
4. Can you (visually) identify a predictor variable $\textbf{x}$ that might be suitable for explaining the response variable $\textbf{y}$?

# Part 2

<details><summary> Open after discussion on zoom</summary>

Here is what I want you to do:

1. Fit a simple linear regression with $\textbf{y}$ as the response variable and separately for each predictor $\textbf{x}$.
2. Use the `confint` function and examine the confidence intervals. How are they interpreted?
3. Fit a multiple linear regression with $\textbf{y}$ as the response variable and with all the predictors $\textbf{x}$ from 1.
4. Compare the parameter estimates and confidence intervals to the results from 1. How have the results changed?
5. What is the interpretation of the intercept? Could centering the covariates change its interpretation?
6. Try including interactions with `x1*x2` or quadratic terms for continuous predictors (with `I(x^2)`) in the model. Can you figure out how to interpret their parameter estimates?
7. Overall, what conclusions can you draw for the relationship of $\textbf{y}$ and $\textbf{x}$?

</details>

[If you finish early you can pick a different dataset, or there is also the practical from last year.](https://htmlpreview.github.io/?https://github.com/BertvanderVeen/GLM-workshop/blob/main/Practicals/2024/3Practical.html)

